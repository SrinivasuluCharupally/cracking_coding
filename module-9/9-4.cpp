// Duplicate URLs: You have 10 billion URLs. How do you detect the duplicate documents? In this
case, assume "duplicate" means that the URLs are identical.`


Ans: 
Solution #1: Disk StorageIf we stored all the data on one machine, we would do two passes of the document. The first pass would split the list of URLs into 4000 chunks of 1 GB each. An easy way to do that might be to store each URL u in afilenamed<x>.txtwherex = hash(u) % 4000.Thatis,wedivideuptheURLsbasedontheirhash value(modulothenumberofchunks).Thisway,allURLswiththesamehashvaluewouldbeinthesamefile.In the second pass, we would essentially implement the simple solution we came up with earlier: load each file into memory, create a hash table of the URLs, and look for duplicates.Solution #2: Multiple MachinesThe other solution is to per rm essentially the same procedure, but to use multiple machines. In this solu­ tion, rather than storing the data in file<x>. txt, we would send the URL to machine x.Using multiple machines has pros and cons.The main pro is that we can parallelize the operation, such that all 4000 chunks are processed simultane­ ously. For large amounts of data, this might result in a faster solution.The disadvantage though is that we are now relying on 4000 different machines to operate perfectly. That may not be realistic (particularly with more data and more machines), and we'll need to start considering how to handle failure. Additionally, we have increased the complexity of the system simply by involving so many machines.
